import os
import logging
import time

import prometheus_client as prom
import cvexporter.openshift_cluster as openshift_cluster
import cvexporter.quay_images as quay  # noqa: F401

from prometheus_client.core import REGISTRY, GaugeMetricFamily


DEFAULT_SKIP_NS = 'dedicated-admin, dedicated-reader, default,' \
                    'kube-public, kube-system, management-infra,' \
                    'openshift, openshift-config, openshift-console,' \
                    'openshift-infra, openshift-logging,' \
                    'openshift-metrics-server, openshift-monitoring,' \
                    'openshift-node, openshift-operator-lifecycle-manager,' \
                    'openshift-operators, openshift-sdn,' \
                    'openshift-web-console, ops-health-monitoring,' \
                    'openshift-psad'


TARGET_NS = os.getenv('TARGET_NS')
SKIP_NS = DEFAULT_SKIP_NS
LISTEN_PORT = int(os.getenv('LISTEN_PORT') or 8080)
POLL_INTERVAL = int(os.getenv('POLL_INTERVAL') or 30)

LOG_LEVEL = str(os.getenv('LOG_LEVEL', 'INFO')).upper()


class CveMetrics(object):
    def __init__(self):
        self.raw_data = []

    def collect(self):
        g = GaugeMetricFamily('container_vulnerabilities_total',
                              'Container Vulnerability Totals by Severity',
                              labels=['namespace',
                                      'saas_context',
                                      'saas_service',
                                      'container_name',
                                      'severity'])
        for ns, context, service, container, sev, vulncount in self.raw_data:
            g.add_metric([ns,
                         context,
                         service,
                         container,
                         sev],
                         vulncount)
        yield g


def merge_metrics(data):
    checked = set()
    merged_data = []
    for ns, sc, ss, cn, sev, val in set(data):
        if not (ns, sc, ss, cn, sev) in checked:
            checked.add((ns, sc, ss, cn, sev))
            vtotal = 0
            for ns2, sc2, ss2, cn2, sev2, val2 in data:
                if (ns2 == ns and sc2 == sc and ss2 == ss and
                        cn2 == cn and sev2 == sev):
                    vtotal = vtotal + val2
            merged_data.append((ns, sc, ss, cn, sev, vtotal))
    return merged_data


def collect_metrics():
    metric_data = []
    for ns, context, service, container, imageurl in \
            openshift_cluster.get_images(TARGET_NS, SKIP_NS):
        if quay.from_quay(imageurl):
            vulnerabilities = quay.get_vulnerabilities(imageurl)
            if vulnerabilities:
                for sev in quay.SEVERITIES:
                    metric_data.append((ns,
                                        context,
                                        service,
                                        container,
                                        sev,
                                        vulnerabilities[sev]))
        else:
            #  logging.info(f'(n:{ns}) {pod}/{container} is not sourced ' +
            #               ' from Quay.io')
            continue
    return merge_metrics(metric_data)


def cache_utilization(cache_stats):
    cache_query_percent = 0
    if cache_stats.hits > 0:
        cache_query_percent = (cache_stats.hits /
                               (cache_stats.hits+cache_stats.misses))
    return ('Cache (size:{}, calls:{}, cache_usage:{:.0%})'
            .format(cache_stats.currsize,
                    cache_stats.hits + cache_stats.misses,
                    cache_query_percent))


def balance_caches():
    logging.info('Quay Vulnerabilities {}'
                 .format(cache_utilization(
                  quay.wget_vulnerabilities.cache_info())))
    quay.wget_vulnerabilities.cache_clear()
    logging.info('Openshift Resources {}'
                 .format(cache_utilization(
                  openshift_cluster.get_resources_by_kind.cache_info())))
    openshift_cluster.get_resources_by_kind.cache_clear()


def main():
    os.environ['TZ'] = 'UTC'
    time.tzset()
    logging.basicConfig(format='%(levelname)s: %(message)s',
                        level=getattr(logging, LOG_LEVEL))
    logging.debug("## DEBUG LOGGING ENABLED ##")
    logging.info('Initializing exporter')
    collector = CveMetrics()
    REGISTRY.register(collector)
    prom.start_http_server(LISTEN_PORT)
    logging.info('Entering collection loop')
    logging.info('Note: Missing labels log once, then occasionally')
    while True:
        collector.raw_data = collect_metrics()
        balance_caches()
        time.sleep(POLL_INTERVAL)


if __name__ == '__main__':
    main()
